## 朴素贝叶斯分类算法流程

```py
import numpy as np


class NaiveBayesClassifier(object):
    def __init__(self):
        """
        self.label_prob表示每种类别在数据中出现的概率
        例如，{0:0.333, 1:0.667}表示数据中类别0出现的概率为0.333，类别1的概率为0.667
        """
        self.label_prob = {}
        """
        self.condition_prob表示每种类别确定的条件下各个特征出现的概率
        例如训练数据集中的特征为 [[2, 1, 1],
                              [1, 2, 2],
                              [2, 2, 2],
                              [2, 1, 2],
                              [1, 2, 3]]
        标签为[1, 0, 1, 0, 1]
        那么当标签为0时第0列的值为1的概率为0.5，值为2的概率为0.5;
        当标签为0时第1列的值为1的概率为0.5，值为2的概率为0.5;
        当标签为0时第2列的值为1的概率为0，值为2的概率为1，值为3的概率为0;
        当标签为1时第0列的值为1的概率为0.333，值为2的概率为0.666;
        当标签为1时第1列的值为1的概率为0.333，值为2的概率为0.666;
        当标签为1时第2列的值为1的概率为0.333，值为2的概率为0.333,值为3的概率为0.333;
        因此self.label_prob的值如下：
        {
            0:{
                0:{
                    1:0.5
                    2:0.5
                }
                1:{
                    1:0.5
                    2:0.5
                }
                2:{
                    1:0
                    2:1
                    3:0
                }
            }
            1:
            {
                0:{
                    1:0.333
                    2:0.666
                }
                1:{
                    1:0.333
                    2:0.666
                }
                2:{
                    1:0.333
                    2:0.333
                    3:0.333
                }
            }
        }
        """
        self.condition_prob = {}

    def fit(self, feature, label):
        """
        对模型进行训练，需要将各种概率分别保存在self.label_prob和self.condition_prob中
        :param feature: 训练数据集所有特征组成的ndarray
        :param label:训练数据集中所有标签组成的ndarray
        :return: 无返回
        """

        # ********* Begin *********#
        unique_c, counts = np.unique(label, return_counts=True)
        self.label_prob = dict(zip(unique_c, counts / len(label)))

        for c in unique_c:
            subset = feature[label == c]
            self.condition_prob[c] = {}
            for i in range(subset.shape[1]):
                unique_x, counts_x = np.unique(subset[:, i], return_counts=True)
                self.condition_prob[c][i] = dict(
                    zip(
                        unique_x,
                        counts_x / subset.shape[0],
                    )
                )
        # ********* End *********#

    def predict(self, feature):
        """
        对数据进行预测，返回预测结果
        :param feature:测试数据集所有特征组成的ndarray
        :return:
        """
        # ********* Begin *********#
        predictions = []
        for f in feature:
            probs = []
            for label in self.label_prob:
                prob = self.label_prob[label]
                for i in range(len(f)):
                    prob *= self.condition_prob[label][i].get(
                        f[i], 1e-6
                    )  # 若不存在，则概率设为一个很小的数
                probs.append(prob)
            predictions.append(np.argmax(probs))
        return np.array(predictions)
        # ********* End *********#
```

## 拉普拉斯平滑

```py
import numpy as np


class NaiveBayesClassifier(object):
    def __init__(self):
        self.label_prob = {}
        self.condition_prob = {}

    def fit(self, feature, label):
        """
        对模型进行训练，需要将各种概率分别保存在self.label_prob和self.condition_prob中
        :param feature: 训练数据集所有特征组成的ndarray
        :param label:训练数据集中所有标签组成的ndarray
        :return: 无返回
        """

        # ********* Begin *********#
        unique_c, counts = np.unique(label, return_counts=True)
        self.label_prob = dict(zip(unique_c, counts / len(label)))

        for c in unique_c:  # 遍历每种类别
            subset = feature[label == c]
            self.condition_prob[c] = {}
            for i in range(subset.shape[1]):  # 遍历每个特征
                unique_x, counts_x = np.unique(subset[:, i], return_counts=True)
                total_count = subset.shape[0]
                self.condition_prob[c][i] = {
                    k: (v / total_count) for k, v in zip(unique_x, counts_x)
                }

                # 拉普拉斯平滑
                all_possible_values = np.unique(feature[:, i])
                for value in all_possible_values:  # 遍历各个取值
                    if value not in self.condition_prob[c][i]:
                        self.condition_prob[c][i][value] = 1 / (
                            total_count + len(all_possible_values)
                        )
                    else:
                        self.condition_prob[c][i][value] = (
                            self.condition_prob[c][i][value] * total_count + 1
                        ) / (total_count + len(all_possible_values))
        # ********* End *********#

    def predict(self, feature):
        """
        对数据进行预测，返回预测结果
        :param feature:测试数据集所有特征组成的ndarray
        :return:
        """
        # ********* Begin *********#
        predictions = []
        for f in feature:
            probs = []
            for label in self.label_prob:
                prob = self.label_prob[label]
                for i in range(len(f)):
                    prob *= self.condition_prob[label][i].get(
                        f[i], 1e-6
                    )  # 若不存在，则概率设为一个很小的数
                probs.append(prob)
            predictions.append(np.argmax(probs))
        return np.array(predictions)
        # ********* End *********#
```

## 新闻文本主题分类

```py
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfTransformer


def news_predict(train_sample, train_label, test_sample):
    """
    训练模型并进行预测，返回预测结果
    :param train_sample:原始训练集中的新闻文本，类型为ndarray
    :param train_label:训练集中新闻文本对应的主题标签，类型为ndarray
    :param test_sample:原始测试集中的新闻文本，类型为ndarray
    :return 预测结果，类型为ndarray
    """

    # ********* Begin *********#
    # 实例化向量化对象
    vec = CountVectorizer()
    # 将训练集中的新闻向量化
    X_train = vec.fit_transform(train_sample)
    # 将测试集中的新闻向量化
    X_test = vec.transform(test_sample)

    # 实例化tf-idf对象
    tfidf = TfidfTransformer()
    # 将训练集中的词频向量用tf-idf进行转换
    X_train = tfidf.fit_transform(X_train)
    # 将测试集中的词频向量用tf-idf进行转换
    X_test = tfidf.transform(X_test)

    clf = MultinomialNB(alpha=0.01)
    clf.fit(X_train, train_label)
    result = clf.predict(X_test)
    return result
    # ********* End *********#
```
